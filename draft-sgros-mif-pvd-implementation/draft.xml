<?xml version="1.0" encoding="US-ASCII"?>
<!-- This template is for creating an Internet Draft using xml2rfc,
     which is available here: http://xml.resource.org. -->
<!DOCTYPE rfc SYSTEM "rfc2629.dtd" [
<!-- One method to get references from the online citation libraries.
     There has to be one entity for each item to be referenced.
     An alternate method (rfc include) is described in the references. -->

<!ENTITY RFC2119 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.2119.xml">
<!ENTITY RFC2629 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.2629.xml">
<!ENTITY RFC3552 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.3552.xml">
<!ENTITY I-D.narten-iana-considerations-rfc2434bis SYSTEM "http://xml.resource.org/public/rfc/bibxml3/reference.I-D.narten-iana-considerations-rfc2434bis.xml">
<!ENTITY I-D.ietf-mif-mpvd-dhcp-support SYSTEM "http://xml.resource.org/public/rfc/bibxml3/reference.I-D.ietf-mif-mpvd-dhcp-support.xml">
<!ENTITY I-D.ietf-mif-mpvd-ndp-support SYSTEM "http://xml.resource.org/public/rfc/bibxml3/reference.I-D.ietf-mif-mpvd-ndp-support.xml">

<!ENTITY RFC6418 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.6418.xml">
<!ENTITY RFC6419 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.6419.xml">
<!ENTITY RFC7556 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.7556.xml">
]>
<?xml-stylesheet type='text/xsl' href='rfc2629.xslt' ?>
<!-- used by XSLT processors -->
<!-- For a complete list and description of processing instructions (PIs),
     please see http://xml.resource.org/authoring/README.html. -->
<!-- Below are generally applicable Processing Instructions (PIs) that most I-Ds might want to use.
     (Here they are set differently than their defaults in xml2rfc v1.32) -->
<?rfc strict="yes" ?>
<!-- give errors regarding ID-nits and DTD validation -->
<!-- control the table of contents (ToC) -->
<?rfc toc="yes"?>
<!-- generate a ToC -->
<?rfc tocdepth="4"?>
<!-- the number of levels of subsections in ToC. default: 3 -->
<!-- control references -->
<?rfc symrefs="yes"?>
<!-- use symbolic references tags, i.e, [RFC2119] instead of [1] -->
<?rfc sortrefs="yes" ?>
<!-- sort the reference entries alphabetically -->
<!-- control vertical white space
     (using these PIs as follows is recommended by the RFC Editor) -->
<?rfc compact="yes" ?>
<!-- do not start each main section on a new page -->
<?rfc subcompact="no" ?>
<!-- keep one blank line between list items -->
<!-- end of list of popular I-D processing instructions -->
<rfc category="info" docName="draft-sgros-an-implementation-of-PvD-support-in-Linux-00">
  <!-- category values: std, bcp, info, exp, and historic
     ipr values: full3667, noModification3667, noDerivatives3667
     you can add the attributes updates="NNNN" and obsoletes="NNNN"
     they will automatically be output with "(if approved)" -->

  <front>
    <title abbrev="An implementation of PvD support in Linux">
		An implementation of PvD support in Linux</title>

    <author fullname="Stjepan Gros"
            initials="S." surname="Gros"
            role="editor">
      <organization>University of Zagreb</organization>
      <address>
        <postal>
          <street>Unska 3</street>
          <city>Zagreb</city>
          <code>10000</code>
          <country>HR</country>
        </postal>
        <phone></phone>
        <email>stjepan.gros@fer.hr</email>
      </address>
    </author>

    <author fullname="Leonardo Jelenkovic"
            initials="L." surname="Jelenkovic"
            >
      <organization>University of Zagreb</organization>
      <address>
        <postal>
          <street>Unska 3</street>
          <city>Zagreb</city>
          <code>10000</code>
          <country>HR</country>
        </postal>
        <phone></phone>
        <email>leonardo.jelenkovic@fer.hr</email>
      </address>
    </author>

    <author fullname="Dejan Skvorc"
            initials="D." surname="Skvorc"
            >
      <organization>University of Zagreb</organization>
      <address>
        <postal>
          <street>Unska 3</street>
          <city>Zagreb</city>
          <code>10000</code>
          <country>HR</country>
        </postal>
        <phone></phone>
        <email>dejan.skvorc@fer.hr</email>
      </address>
    </author>

    <date month="March" year="2016" day="5" />

    <!-- Meta-data Declarations -->

    <area>General</area>

    <workgroup>Internet Engineering Task Force</workgroup>

    <!-- WG name at the upperleft corner of the doc,
         IETF is fine for individual submissions.
     If this element is not present, the default is "Network Working Group",
         which is used by the RFC Editor as a nod to the history of the IETF. -->

    <keyword>mpvd</keyword>
    <keyword>ra</keyword>
    <keyword>ndp</keyword>
    <keyword>mif</keyword>
    <keyword>implementation</keyword>

    <abstract>
      <t>
      The purpose of this draft is to document two implementations that tried
      to implement PvD architecture document. One implementation was done from
      scratch (PvD-manager) while another is a part of an existing component
      (NetworkManager).
      </t>
    </abstract>
  </front>

  <middle>

    <section title="Introduction">

      <t>
      TBD.
      </t>

      <section title="Requirements Language">
        <t>The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
        "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
        document are to be interpreted as described in <xref
        target="RFC2119">RFC 2119</xref>.</t>
      </section>

    </section>

    <section title="Common implementation mechanism">

      <t>
      In this section we describe some common implementation concepts.
      Basically, they follow from the preparation phase when different
      approaches were analysed. 
      </t>

      <t>
      First of, the implementations are all done on modern Linux operating
      system, more specifically, Fedora Desktop distribution. The main
      characteristic of this distribution is that the main IPC mechanism
      is D-Bus. So, D-Bus is also used by the two implementations described
      in this draft.
      </t>

      <t>
      Next, in the analysis phase the implementations assessment was
      done to determine which mechanisms to use with the main goal of
      having control over the use of provisioning domains by applications.
      In other words, the main issue was how to control which PvD will
      an application use in the presence of multiple provisioning domains.
      The problems we tried to address are well described in <xref
      target="RFC6418">RFC 6418</xref>. The conclusion was that the
      network namespaces are the most appropriate mechanism to control
      application use of provisioning domains.
      </t>

      <t>
      Still, while network namespaces were found to be the most appropriate
      machanism for separation there is a big issue in that it is not possible
      to have the same PvD instance in separate network namespaces. To get
      around that issue multiple PvD instaces could be used by a single node.
      Yet, this is not without the problems either. First, from the operational
      perspective this leads to proliferation of IP addresses and to issues
      with accountability. It is even a bigger issue on IPv4 networks that have
      restricted number of PvD instances per each PvD.
      </t>

    </section>

<section title="PvD-manager Implementation">
<t>
	PvD-manager is a client-side component for IPv6 network auto-configuration based
	on multiple provisioning domains (PvDs), as described in
	<xref target="RFC6418">RFC 6418</xref> and <xref target="RFC7556">RFC 7556</xref>.
	PvD-manager is orthogonal to existing system, which means it does not interfere
	with regular network behavior: none of the services and settings used by Network
	Manager and similar components are not changed nor affected.
	Implementation and documentation for PvD-manager is available at its
	<xref target="PvD-manager">git repository</xref>.

</t>

<section title="Architecture">
<t>
	PvDs are implemented through Linux network namespaces. For each coherent PvD
	information set received on the network interface, PvD-manager creates a separate
	network namespace and configures received network parameters within that
	namespace. Since each network namespace uses separate IP stack which is isolated
	from other namespaces, potentially conflicting network parameters received from
	different network providers can safely coexist on a single host. PvD-manager
	manages only those newly created namespaces associated with the PvDs and their
	network settings and leaves the default network namespace intact. That way, all
	the existing network management components, such as Network Manager, continue to
	work unobtrusively.
</t>
<t>
	<xref target="PvD-manager-architecture" /> presents an overview of a system
	and its components which uses PvD-manager.
</t>
<figure anchor="PvD-manager-architecture" title="PvD prototype architecture overview">
<artwork><![CDATA[
+---------------------------------------------------+  +-------------+
| +-----------------------------------------------+ |  |   +-------+ |
| | PvD-manager                                   | |  | +-+ radvd | |
| | +-----------+   +-----------+   +-----------+ | |  | | +-------+ |
| | | pvdserver +->-+   pvdman  +-<-+ ndpclient | | |  | |           |
| | +-----+-----+   +-------+---+   +-----+-----+ | |  | | +-------+ |
| +-------|-----------------|-------------|-------+ |  | +-+ httpd | |
|         |d-bus      create|configure    |RA/RS    |  | | +-------+ |
| get_pvds|           delete|             |HTTP     |  | |           |
| +-------|-------+    +----+---------+   |         |  | |           |
| | +-----+-----+ |join|   network    |  -+---------+--+-+--        -+-
| | |  MIF API  +------+  namespace   |             |  |             |
| | +-----------+ |    |  operations  |             |  |             |
| |               |    +--------------+             |  |             |
| |   PvD aware   |                                 |  |             |
| |  application  |                                 |  |             |
| +---------------+                    Client (PC)  |  |     Router  |
+---------------------------------------------------+  +-------------+
]]></artwork>
</figure>
<t>
	PvD-manager receives network configurations through Router Advertisement (RA)
	messages. Modified version of PvD aware
	<xref target="radvd">radvd daemon</xref> is used. Each RA may contain one or
	more network configurations which are classified either as explicit or implicit
	PvDs. Explicit PvD is a set of coherent NDP options explicitly labeled with a
	unique PvD identifier and nested within a special NDP option called PVD
	container, as described in <xref target="I-D.ietf-mif-mpvd-ndp-support">
	draft-ietf-mif-mpvd-ndp-support-02</xref>. Multiple
	explicit PvDs may appear in a single RA, each within a different PvD container
	option, as long as they are labeled with different PvD identifiers. Implicit PvD
	is just another name for top-level NDP options placed outside the PvD container
	option, as in regular PvD unaware router advertisements. Since implicit PvDs
	are not labeled with PvD identifier, PvD-manager automatically generates an
	identifier for internal use and configures the implicit PvD on the host in the
	same way as if it was the explicit one. Only one implicit PvD is allowed per RA.
	In current prototype, UUID is used as a PvD identifier.
</t>
<t>
	Each PvD, either explicit or implicit, is associated with a network namespace
	with a single virtual network interface (besides the loopback) of macvlan type,
	where PvD-related network parameters are configured. To establish a connectivity
	to the outside world, virtual interface is connected to the physical interface
	on which the related PvD information is received through the RA. Each virtual
	interface is assigned a link-local IPv6 address (fe80::/64) and one or more
	addresses derived from Prefix Information options if present in the received RA.
	Besides the IP addresses, PvD-manager configures the routing tables and DNS
	records within the namespace. By default, a link-local and default route via the
	RA-announcing router are added to the routing table, regardless of the routing
	information received in RA. Additional routing information is configured if
	Route Information options are received in RA.
	Finally, for each RDNSS and DNSSL option received in RA, PvD-manager creates a
	record in /etc/netns/NETNS_NAME/resolv.conf, where NETNS_NAME is a name of the
	network namespace associated with the PvD.
</t>
<t>
	PvD aware client application uses PvD API to get a list of available PvDs
	configured on the local host, and activate chosen PvD to use it for
	communication. Information about configured PvDs are exposed to applications by
	a special PvD service running on local host. D-Bus is used to connect
	applications to PvD service. Upon PvD activation, client application is switched
	to the network namespace associated with the selected PvD. Further network
	operations (socket creation, sending and receiving data) are performed within
	that namespace. Once obtained by the application, socket handles are linked to
	the network namespace they were originally obtained from and continue to work in
	that namespace, regardless of whether the application switches to another
	namespace at some time later. This enables the application to use multiple PvDs
	simultaneously. The only requirement is that the application is running within a
	proper network namespace while obtaining a socket.
</t>
<t>
	PvD unaware clients operate as before. Although they are not able to use PvD
	API to select a certain PvD, they can still be forced to use a specific PvD by
	starting them in a network namespace associated with that PvD. To run a program
	within a given namespace, it should be started with:
	<list style="empty">
		<t>ip netns exec &lt;pvd-namespace-name&gt; &lt;command with args&gt;</t>
	</list>
	or they can be started with provided launchers ("pvd_run" and "pvd_prop_run").
</t>
<t>
	As per <xref target="RFC7556">RFC 7556</xref>, implemented PvD system
	provides basic, intermediate and advanced PvD support (in APIs) for
	client applications. Only difference is that our basic support doesn't
	provide automatic selection for PvD unaware application - it must be started
	with PvD launcher with manual selection of PvD. Intermediate and advanced
	PvD support require some additional properties (metadata) provided with PvD.
	Next section describe used mechanism to provide such information to PvD-manager
	which then provide such information to client applications.
</t>

</section>

<section title="PvD Properties">
<t>
	With RA messages routers provides network related parameters for PvDs.
	Other parameters that can be used to detail properties of particular PvD
	(an application can use them to better select PvD) in this draft are called "PvD
	properties" or just "properties".
</t><t>
	In this prototype implementation, PvD properties are also provided by
	router, but only on request, using HTTP protocol on router's link-local address,
	using port 8080. Router's link-local address is saved by PvD-manager when RA was
	received.
</t><t>
	Upon receiving PvD information from router, PvD-manager tries to get a file with
	PvD properties from the same router. If such file exists, network related PvD
	parameters are extended with ones (properties) from received file.
</t><t>
	Client application receives all those additional properties from PvD-manager,
	and may select appropriate PvD based on them.
</t><t>
	Current implementation is very rudimentary: files on router are in JSON format.
	PvD-manager interpret them - create a dictionary of them, but only because
	PvD-manager it's written in Python and using JSON is easy, while client
	applications are written in C. In real implementation this should be reversed -
	only client should interpret file with PvDs' properties.
</t><t>
	Properties used in this prototype are just an example ("name", "type", "bandwidth",
	"price"), not to be used in some protocol specification.
	We presented one mechanism to provide additional PvD properties obtained
	by some mechanism (not by RAs) and let client application decide what to do with
	them.
</t>

<t>
	<xref target="PvD-property-examples" /> presents example properties for several
	PvDs when obtained from two routers (R1 and R2 from test scenarios described in
	<xref target="test-scenarios" />.
</t>
<figure anchor="PvD-property-examples" title="PvD property examples">
    <artwork><![CDATA[
From R1:
[
    {
        "name": "Home internet access",
        "type": ["internet", "wired"],
        "id": "implicit",
        "bandwidth": "10 Mbps",
        "pricing": "free"
    },
    {
        "name": "TV",
        "type": ["iptv", "wired"],
        "id": "f037ea62-ee4f-44e4-825c-16f2f5cc9b3e",
        "bandwidth": "10 Mbps",
        "pricing": "free"
    }
]
From R2:
[
    {
        "name": "Cellular internet access",
        "type": ["internet", "cellular"],
        "id": "implicit",
        "bandwidth": "1 Mbps",
        "pricing": "0,01 $/MB"
    },
    {
        "name": "Phone",
        "type": ["voice", "cellular"],
        "id": "f037ea62-ee4f-44e4-825c-16f2f5cc9b3f",
        "bandwidth": "0,1 Mbps",
        "pricing": "0,01 $/MB"
    }
]
]]></artwork>
</figure>
</section>

<section title="Deployment">
	<!--<section title="Routers, DNS Servers">-->
<t>
	PvD architecture assumes presence of at least one router which runs
	modified version of <xref target="radvd">radvd daemon</xref>, described
	in <xref target="section-radvd"/>.
	Through RA messages, router conveys network related parameters to client
	host (prefix, routes, DNS servers and domains).
	Router should also provide PvD properties, using an HTTP server on port 8080
	attached to router's link-local IP address.
</t>
<t>
	DNS servers aren't part of PvD architecture but could be used to demonstrate
	that different PvDs can use different DNS servers.
</t>
<!--</section>
<section title="PvD-manager, Client Applications">-->
<t>
	PvD-manager is a daemon for client host. Currently PvD-manager consists of
	several modules. Main module maintain PvD information, creates, updates and
	deletes namespaces. NDP module listens for RA messages, parse them and forward
	them to main module. API server module listens for client application
	requests (using d-bus) and responds to them, and also send signals to clients
	when a change occurred in PvDs.
</t>
<t>
	Before starting a network connection, PvD aware client application should first
	request PvDs from PvD-manager. Next, one PvD should be selected (activated)
	and only then network connection(s) created.
	If other PvDs are required (later or in parallel), same procedure must be
	followed: select PvD first and then create connections.
	Connection will continue to operate within PvD in which was created,
	regardless of PvDs selected later.
</t>
<t>
	PvD unaware application should be started with PvD launcher to use certain
	PvD. Otherwise, such application will behave as PvDs weren't present
	("as usual").
</t>
<!--</section>-->
</section>

<section title="Implementation Details">

<t>
	Proposed PvD architecture is based on Linux namespaces as PvD isolation
	mechanism. Isolation namespaces provide resolve many issues about
	overlapping and conflicting network parameters for different PvDs.
	However, they also impose some requirements that may limit usage in certain
	environment, especially ones based on public IPv4 addresses.
	One of the main problem with namespaces is that each namespace requires
	an IP address (since namespace emulates network from link-layer and up).
</t>
<t>
	Only IPv6 is used in implementation. Main reasons include using RA messages
	as PvD information provider and unrestricted generation of IPv6 address per
	PvD.
</t>

<t>
	A library is created with interfaces (API) for PvD operations, currently only for
	programs in C programming language.
	For communication between client application (using provided API) and
	PvD-manager d-bus service is used.
	Implemented interface include PvD retrieval methods
	(pvd_get_by_id, pvd_get_by_properties), PvD selection (pvd_activate) and
	registration for events when PvDs change (pvd_register_signal).
	Sample test applications which
	demonstrate API usage and PvD system possibilities are provided in repository.
</t>

<t>
	PvD-manager is implemented in Python 3 because it allows rapid prototyping
	using network managing modules (mainly pyroute2 and netaddr).
</t>


<t>
	More details about implementation of PvD-manager is available in its
	<xref target="PvD-manager">documentation</xref>.
</t>
</section>

<section title="Test Scenarios" anchor="test-scenarios">

<t>
	Test scenarios used for validating implementations include a system with
	one client host, two routers and two hosts that act as servers, as presented
	on <xref target="demo-system" />.
	All hosts are running as virtual machines.
</t>
<figure anchor="demo-system" title="Network configuration used in test scenarios">
<artwork><![CDATA[
              fd01::1/64+----+                                   +----+
        2001:db8:1::1/64|    |2001:db8:10::1/32 2001:db8:10::2/32|    |
           +----------o-+ R1 +-o-------------------------------o-+ S1 |
+------+   |            |    |        :     [VMnet3]             |    |
|      |   |            +----+        :                          +----+
|Client+-o-+ [VMnet2]                 : (for some tests this is linked)
|      |   |            +----+        :                          +----+
+------+   |            |    |        :     [VMnet4]             |    |
           +----------o-+ R2 +-o-------------------------------o-+ S2 |
        2001:db8:2::1/64|    |2001:db8:20::1/32 2001:db8:20::2/32|    |
              fd02::1/64+----+                                   +----+
]]></artwork>
</figure>

<t>
	All server hosts, including routers have configured HTTP and DNS servers
	providing many possibilities for testing. Routers, in RAs advertise prefixes
	as shown on <xref target="demo-system" />. Local addresses are used in
	explicit PvDs (simulating some specific service), while public in implicit.
</t>
<t>
	Example network configurations from <xref target="RFC7556">RFC 7556</xref>
	are simulated with <xref target="demo-system" /> with various applications
	on Client and servers S1 and S2.
	S1 is accessible by client only through implicit PvD provided
	by R1, while S2 similarly, only over PvD provided by R2.
</t>
<t>
	If S1 simulate one service, and S2 another, client application can select
	PvD based on service required and connect to S1 or S2. Or a PvD aware
	application can use both in parallel.
</t>
<t>
	VPN was simulated by a tunnel between Client and S2, created within
	implicit PvD provided by R2. Then tunnel was added as another PvD.
	S2 for this scenario had local address (and prefix) as local addresses
	on R2 network: there were two PvDs with same prefix on Client.
	However, client applications running in those two different PvDs for the
	same IP address (fd02::1) connected with different servers: one with
	R2, and another (which used "VPN" PvD) with S2.
</t>
<t>
	In some scenarios both S1 and S2 were connected to both routers R1 and R2.
	In this scenarios better PvD could be chosen for connecting with servers,
	if properties for PvDs are provided. Also, when some connection fails - when
	some PvD loses connectivity, application can reset connections, refresh
	PvD availability from PvD-manager and chose again among active PvDs.
</t>
<t>
	More details for described scenarios (and some other)
	are provided in <xref target="PvD-manager">demonstration test cases
	</xref>.
</t>
</section>

<section title="Experiences gained">
<t>
	Present some experience gained from the implementation. What
	do you think was good? What would you do differently if you
	were to implement this again. What about language/environment,
	was it good, what was bad?
</t>
<section title="Linux namespaces">
<t>
	Using Linux namespaces still seems best option for PvD realization despite
	its drawbacks. Its isolation and ease of use from PvD-manager and
	client application perspective can hardly be outmatched with other solutions.
	Besides already mentioned need for separate IP address per
	namespace, there are several more issues with namespaces.
</t>
<t>
	Managing namespaces (creation, deletion, modification) requires root
	privileges (as expected). However, even switching an application from one
	namespace to another is possible only if application has root privileges.
	This currently limits this namespace approach to only applications run by
	root. For lifting this limitation, changes in Linux kernel and namespace
	handling is required. Some sort of permission system should also be applied to
	namespaces (e.g. similarly with permissions on files and system objects).
</t>
<t>
	Switching namespace from within the application with setns doesn't update DNS
	related configuration as expected. When an application is started with commands
	"ip netns exec &lt;namespace-name&gt; &lt;application&gt; [arguments]",
	DNS configuration is updated (/etc/resolf.conf is the one from
	/etc/netns/&lt;namespace-name&gt;/). However, setns doesn't replicate that
	behavior, and that manipulations should be done separately (mounting certain
	directories/files).
</t>
<t>
	When a namespace is created, a virtual device is created that is linked
	with physical device (and it gets assigned its own MAC address). However, if
	on particular physical device can't be linked virtual one (like VPN), then
	either physical device must be moved to certain PvD or some sort of bridge
	created and devices attached to that bridge that can be moved to namespace.
	Sometimes, it's better to move physical device to particular namespace (PvD)
	and allow only some applications its usage (e.g. like VPN).
</t>
<t>
	Managing namespace: creating, deleting, adding device to it, adding ip address
	and routes are operations performed within PvD-manager and they aren't instant.
	Maybe that is expected and "normal" since such operations aren't to be
	performed frequently (only when something in network changes). However, when
	testing frequent changes in PvDs (routers were connected and disconnected)
	significant delay in PvD aware application was detected. Sometimes PvD-manager's
	API server module (that is responsible for client communication) become
	unresponsive for at least several seconds. This could be a bug in PvD-manager
	or result of changes PvD-manager sent being applied by Linux kernel.
</t>
<t>
	Recently added feature of Linux kernel (January, 2016), named
	<eref target="https://www.kernel.org/doc/Documentation/networking/vrf.txt">
	Virtual Routing and Forwarding (VRF)</eref>, seems
	possible alternative to namespaces. However, it should be investigate further
	to create some conclusions.
</t>

</section>
</section>

</section>

    <section title="NetworkManager implementation">

      <t>
      NetworkManager is a software component used in modern Linux
      distributions to control network connections. It runs as a
      daemon tracking and reacting to the network related events,
      either those coming from the network (like, for example, Router
      Advertisements) or those from the local system (e.g. attachment
      of a new networking device). Furthermore, it exposes certain
      methods and properties over D-Bus interface so that it can be
      controlled by different clients, the most prominent being network
      manager applet and nmcli command line tool.
      </t>

      <t>
      Due to its importance in the modern Linux distributions it was
      valuable experience to try to implement PvDs within it. Yet,
      NetworkManager is a very complex piece of a software that wasn't
      designed with PvDs in mind so it wasn't straightforward task to
      try to implement them and there were some difficulties that had
      to be overcome. In the following text we'll first describe how
      NetworkManager behaves now with respect to multiple PvDs, then
      we'll describe one approach on how to add support for multiple
      PvDs.
      </t>

      <section title="NetworkManager's Current Behavior">

        <t>
        In this section we describe how unmodified NetworkManager
        behaves with respect to multiple provisioning domains.
        </t>

        <t>
        First, we have to state that NetworkManager doesn't use network
        namespaces, that is, everything is kept in one network namespace
        that we'll call "root network namespace" or "main network namespace".
        So, all devices and all the configuration parameters are in one place.
        </t>

        <t>
        The situations in which NetworkManager handles multiple provisioning
        domains are:
        </t>

        <t><list style="hanging" hangIndent="6">

          <t hangText="VPN Connections.">
          <vspace />
          When user activates VPN connection upon successful establishment
          of a connection, NetworkManager receives PvD instance for the given
          VPN connection. In some cases after establishing VPN there is
          virtual device present (e.g. OpenVPN), while in other cases there
          are no new devices but only appropriate additions to the existing
          IP packet processing path (e.g. IPsec). In any case, interface and
          the associated PvD instance are assigned and present in the root
          network namespace. Furthermore, DNS data is also merged with
          existing data. It isn't hard to see that this mixing of PvD
          instances might lead to very complex situations in which it is hard
          to control which PvD instance will be used. One possibility to
          control what will be accessible is default route. Namely, it is
          possible to instruct NetworkManager not to install default route
          via VPN PvD instance even though it was sent by VPN gateway.
          </t>

          <t hangText="Concurrent active wired and wireless connections.">
          <vspace />
          Again, like in the previous case, all the settings are mixed and
          this mixing has a consequence of not be able to use the two
          connections in parallel. Current policy that is enforced by
          NetworkManager is to prefer wired connection for default route.
          In other words, unless there are more specific routes that use
          wireless connection, it will be not used while wired network
          connection is used.
          </t>

          <t hangText="Multiple IPv6 routers on a local network.">
          <vspace />
          This is interesting use case even though it is not as common now.
          Namely, there is a possibility that there are two IPv6 capable
          routers on the local network that overlap in connectivity.
          What NetworkManager will do in this case is that all PvDs received
          in RAs will be merged together per device.
          </t>

          <t hangText="Multiple concurrent DNS servers on a local network.">
          <vspace />
          This is a final use case that isn't possible by the current
          protocol design. Namely, DHCP offers received are treated as
          alternative and there is no way to have multiple DNS servers
          on a single local network.
          </t>

        </list></t>

      </section>

      <section title="The architecture and components">

        <t>
        We can describe NetworkManager in terms of static and runtime
        architecture. The static architecture is reflected by the source
        code organization. For the purpose of this draft will only
        present NetworkManager itself, but take into account that there
        are additional components, too. So, the NetworkManager's code is
        in src/ directory of top-level NetworkManager directory. There
        you'll find the following subdirectories:
        </t>

        <t><list style="hanging" hangIndent="6">

          <t hangText="devices/">
          <vspace />
          Objects that control devices.
          </t>

          <t hangText="dhcp-manager/">
          <vspace />
          </t>

          <t hangText="dns-manager/">
          <vspace />
          </t>

          <t hangText="dnsmasq-manager/">
          <vspace />
          </t>

          <t hangText="platform/">
          <vspace />
          </t>

          <t hangText="ppp-manager/">
          <vspace />
          </t>

          <t hangText="rdisc/">
          <vspace />
          </t>

          <t hangText="settings/">
          <vspace />
          </t>

          <t hangText="supplicant-manager/">
          <vspace />
          </t>

          <t hangText="systemd/">
          <vspace />
          </t>

          <t hangText="vpn-manager/">
          <vspace />
          </t>

        </list></t>

      </section>

      <section title="Implementation">

        <t>
        To understand NetworkManager it is first necessary to understand
        GObject system which is created so that it is possible to use
        object oriented programming in the C programming language, and
        also to allow easy integration with D-Bus.
        </t>

        <t>
        The implementation of PvD support within NetworkManager was done
        with the following requirements in mind:
        </t>

        <t><list>

          <t>
          For backwards compatibility there should be a root network
          namespace which is handled by NetworkManager as before, i.e.
          it contains all configurations received merged into one.
          </t>

          <t>
          NetworkManager doesn't touch network namespaces created by
          other applications, like for example different virtualization
          solutions.
          </t>

        </list></t>

        <t>
        The main objects that NetworkManager is based on are devices,
        connections and properties.
        </t>

        <section title="Network namespace management implementation">

          <t>
          To support network namespace management it was necessary to
          add two objects: NMNetworkNamespaceController which is a
          singleton object that is used to enumerate available
          network namespaces, create a new one or remove existing
          network namespace.
          </t>

          <t>
          The second object introduced to allow NetworkManager support
          network namespaces is NMNetns. For each network namespace
          created by NetworkManager one NMNetns object is created. This
          object has an interface exposed over D-Bus that allows one to
          query all available devices within the network namespace, to
          take device from some other network namespace and to activate
          connections.
          </t>

        </section>

        <section title="Provisioning domain support implementation">

          <t>
          </t>

        </section>

      </section>

    </section>

    <section title="Server component" anchor="section-radvd">

      <t>
      Write about changes in the radvd component.
      </t>

    </section>

    <section anchor="Acknowledgements" title="Acknowledgements">
      <t>
      TBW.
      </t>
    </section>

    <!-- Possibly a 'Contributors' section ... -->

    <section anchor="IANA" title="IANA Considerations">
      <t>This memo includes no request to IANA.</t>

      <t>All drafts are required to have an IANA considerations section (see
      <xref target="I-D.narten-iana-considerations-rfc2434bis">the update of
      RFC 2434</xref> for a guide). If the draft does not require IANA to do
      anything, the section contains an explicit statement that this is the
      case (as above). If there are no requirements for IANA, the section will
      be removed during conversion into an RFC by the RFC Editor.</t>
    </section>

    <section anchor="Security" title="Security Considerations">
      <t>All drafts are required to have a security considerations section.
      See <xref target="RFC3552">RFC 3552</xref> for a guide.</t>
    </section>
  </middle>

  <!--  *****BACK MATTER ***** -->

  <back>
    <!-- References split into informative and normative -->

    <!-- There are 2 ways to insert reference entries from the citation libraries:
     1. define an ENTITY at the top, and use "ampersand character"RFC2629; here (as shown)
     2. simply use a PI "less than character"?rfc include="reference.RFC.2119.xml"?> here
        (for I-Ds: include="reference.I-D.narten-iana-considerations-rfc2434bis.xml")

     Both are cited textually in the same manner: by using xref elements.
     If you use the PI option, xml2rfc will, by default, try to find included files in the same
     directory as the including file. You can also define the XML_LIBRARY environment variable
     with a value containing a set of directories to search.  These can be either in the local
     filing system or remote ones accessed by http (http://domain/dir/... ).-->

    <references title="Normative References">
      <!--?rfc include="http://xml.resource.org/public/rfc/bibxml/reference.RFC.2119.xml"?-->

      &RFC2119;

      &RFC6418;

      &RFC6419;

      &RFC7556;

    </references>

    <references title="Informative References">
      <!-- Here we use entities that we defined at the beginning. -->

      &RFC2629;

      &RFC3552;

      &I-D.narten-iana-considerations-rfc2434bis;

      &I-D.ietf-mif-mpvd-dhcp-support;

      &I-D.ietf-mif-mpvd-ndp-support;

      <!-- A reference written by an organization not a person. -->

    </references>
    <references title="Implementation repositories">
		<reference anchor="PvD-manager" target="https://github.com/l30nard0/mif">
		<front>
			<title>PvD-manager repository</title>
			<author fullname="Leonardo Jelenkovic"
					initials="L." surname="Jelenkovic"
					>
			<organization>University of Zagreb</organization>
			</author>

			<author fullname="Dejan Skvorc"
					initials="D." surname="Skvorc"
					>
			<organization>University of Zagreb</organization>
			</author>

			<date month="March" year="2016" />
		</front>
		</reference>
		<reference anchor="radvd" target="https://github.com/dskvorc/mif-radvd">
		<front>
			<title>PvD customized radvd daemon</title>
			<author fullname="Dejan Skvorc"
					initials="D." surname="Skvorc"
					>
			<organization>University of Zagreb</organization>
			</author>

			<date month="February" year="2016" />
		</front>
		</reference>
    </references>

  </back>
</rfc>
